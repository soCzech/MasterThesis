\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

% For humans, vision is the most important source of information hence it is no surprise that video is considered as the richest type of multimedia available.
For humans, vision plays the most important role in building representation of the surrounding environment. We rely on sight heavily -- some even estimate that information from our eyes accounts for eighty percent of stimuli from the environment. It is, therefore, no wonder that, with the help of cheap recording devices that everybody carries in their pockets, we have become obsessed with capturing what we see.

Smartphones enabled us to record every moment of our lives, and personal archives of photos and videos started growing rapidly. With the rise of social networks and the internet in general, we began not only capturing photos and videos but also sharing them online. In 2013 it was reported that just only Facebook's databases contained 250 billion photos with 350 million new photos added every day by its users\footnote{\url{https://www.theverge.com/2013/9/17/4741332}} -- a figure that has probably grown since. Video platform YouTube announced in 2019 that it had been receiving more than 500 hours of video every minute\footnote{\url{https://www.cnbc.com/2018/03/14/with-over-1-billion-users-heres-how-youtube-is-keeping-pace-with-change.html}}. To put that in context, humans live shorter lives than the length of videos uploaded to the service every day.
% the length of videos uploaded in a day exceeds the length of human life.

With such amount of multimedia being recorded, new problems and challenges arise. Given the enormous sizes of collections, it is impossible to sequentially browse through the data. Efficient methods for work with multimedia collections need to be utilized. The use-cases for these methods vary from a plethora of methods for searching and transcribing the content to the summarization of the individual parts of the collections \cite{zhang2019pegasus_textsum,Miech_2019_ICCV_HowTo100M,zhou2018deep_videosum}.
Multimedia in the collections are, however, usually not manually annotated, and contain only basic metadata such as date, time, and location, if any at all.

In recent years, thanks to deep learning, we have seen huge improvements in many areas, including automatic annotation of images, videos, and other types of multimedia. Yet video, one of the richest type of multimedia, still presents multiple challenges, such as its enormous size. Compared to other types of media like text, audio, or images, video sizes are whopping -- a short clip can easily require a hundred times as much space as a single image. Comparison is even starker in the case of audio and text. As the deep learning-based approaches usually require large datasets, applying deep learning on video-related tasks depends upon the collection of harder to obtain and more time consuming to annotate datasets when compared to their image counterparts. Furthermore, training of the models requires more computational power and time due to the increased dimensionality of the problem.

A popular approach to circumvent the need for large training datasets or lack of computational power in end-to-end deep learning is to decompose a problem into multiple sub-problems, solving each independently. In the video domain, that means, for example, to extract image features from each frame and train a model utilizing only the extracted features instead of the high dimensional frames themselves \cite{Miech_2019_ICCV_HowTo100M,XirongW2VVpp}. In the domain of self-driving cars, one can create a model predicting a depth map from multiple images \cite{Yin_2019_CVPR_HD3} and then another model for obstacle detection using the images enriched with their depth \cite{Qi_2018_CVPR_FrustumPointNets}. In text-based systems, it is not uncommon to build on top of Word2Vec-like \cite{word2vec} pre-trained embeddings instead of training embeddings from scratch \cite{Miech_2019_ICCV_HowTo100M,XirongW2VVpp}. In general, the less training data there is, the more likely there will be benefits to utilizing the multi-step approach.

Many video-related methods take the decomposition approach to an extreme by discarding any temporal information from a video and working only with single frames or simply averaging multiple frames' features \cite{Miech_2019_ICCV_HowTo100M,XirongW2VVpp}. Surprisingly, until very recently \cite{Miech_2020_CVPR}, these methods dominated many video related benchmarks, probably due to lack of annotated video data.
% Until very recently methods utilizing only individual video frames dominated many video related benchmarks due to lack of annotated video data.
Strangely, even with the introduction of large annotated video datasets \cite{Ray_2018_ECCV}, we do not see such a sharp boost in performance, which can be seen in the image domain. Some theorize it is in part due to vagueness and ambiguity of actions observed in videos. Action \textit{`playing tennis'} can for different people mean vastly different clips. TV broadcasts from Wimbledon, table tennis tournament with friends in a basement, a kid hitting a ball in a backyard, or a computer game are all valid alternatives. 
Even though there is also ambiguity between objects, it is usually less pronounced.

To overcome the ambiguity and to correct errors of automatic methods, there has been research in human-assisted approaches \cite{VBS2019_overview}. They revolve around assisted browsing in the collections by utilizing novel user interfaces \cite{DuaneGurrin_VirtualReality_LSC2018}, hierarchical collection maps \cite{KlausWikiView}, or iterative query refinement by positive and negative examples \cite{Exquisitor_VBS2020,SOMHunterVBS2020}.
However, a comparison of such approaches is difficult because a person needs to be present in the evaluation loop. In recent years competitions such as Video Browser Showdown (VBS) \cite{VBS2019_overview} or Lifelog Search Challenge (LSC)~\cite{LSC2020} emerged to accelerate research in human-assisted approaches, in particular in a task of known-item search.
The known-item search (KIS) task represents a situation where a user searches for a given item (usually an image or a segment from a video) in a large collection of data.
%, e.g. a personal photo or video collection. In case of video KIS task where an user searches for a short clip
With an increasing amount of multimedia content we generate, there is a wide range of image or video collections where know-item search scenario may play an important role -- a personal photo or video archive, footage from CCTV cameras, databases of news clips or medical videos to name a few.

% Over the years the mentioned competitions served as a stage for evaluation of state-of-the-art tools for know-item search.
Over the years, Video Browser Showdown served as a stage for an evaluation of many known-item search approaches in large video collections. These are important concepts mentioned by winning teams:
\begin{description}[labelwidth=1em, leftmargin=!]
	\item \textbf{Powerful query initialization.} It is beneficial to limit search space by an initial query that filters out most of the unrelated items or, with enough luck, immediately discovers the searched video segment. For many years, color \cite{BlazekAdam2015} or edge sketches were widely used; however, these are useful only if a user knows the exact visual representation of a searched scene. Further, with advances in deep learning, concept and text search replaced sketches as it is a more effective approach \cite{LokocVBS2020_VIRET, vitrivr2020}. Lastly, note that the initial query also plays an important role in many query refinement methods introduced in the next paragraph as they require negative but also positive samples, which are hard to gather without good initialization.
	% is used to gather positive examples for query refinement.
	% It is beneficial to start iterative search with high chance .
	% No matter what query refinement method is used, large collections require 
	% Large databases require a method to reduce focus to only a part of the collection. Text search, search by example but text search dominates. Also Filters.
	\item \textbf{Effective query refinement.} In large collections with a lot of similar content, it is unlikely that the searched scene will be found on the first try with a query. Many KIS tools support either assisted text query reformulation based on presented results or encourage users to select positive and negative examples to further narrow and rearrange the result set \cite{Exquisitor_VBS2020,SOMHunterVBS2020}. Also, `find similar' function is widely used to retrieve similar content from the whole collection \cite{LokocVBS2019_Nasnet, vitrivr2020} -- nowadays usually implemented as nearest-neighbor search in high-dimensional representations of the content computed by deep neural networks.
	% Recently Bayesian approach that displays images based on their probability given positive and negative samples proved to be useful.
	\item \textbf{Fast assisted browsing.} Browsing is utilized if a scene is not found using only the approaches mentioned above. %It is strongly tied to query refinement since the refinement commonly requires good results to start from.
	Good browsing approaches should exploit information from initial query results but also consider further exploration. Some methods of browsing involve computation of 2D image maps \cite{KlausVBS2019_sketchsearch, KlausWikiView}, and some utilize the power of virtual reality \cite{DuaneGurrin_VirtualReality_LSC2018}. Recently, Bayesian approach that samples images to display based on their probability proved to be successful \cite{SOMHunterVBS2020}.
	\item \textbf{Intuitive user interface.} The tools are operated by not only the authors but also by novice users. A cluttered user interface or hard-to-understand retrieval models result in lower performance of a tool when operated by novices.
\end{description}
%There is also another key part of all KIS tools -- key frame selection. It is not mentioned in related work much since collections used at competitions usually come with pre-selected set of key frames using some heuristics.
\begin{description}[labelwidth=1em, leftmargin=!]
	\item \textbf{Key frame selection.} In general, videos may be too long and may contain different unrelated scenes. Therefore, the search is often performed on shorter video segments. Usually, a segment of a video is represented by a single frame (keyframe). The selection of the segment and its keyframe can be performed by thresholding differences between (multiple) adjacent frames and their visual features \cite{V3C1dataset}. However, setting threshold too low results in oversampling, which increases database size and clutters result lists. Too high threshold causes some unique video segments to be missed. Recently more accurate shot boundaries, detected by deep learning-based methods \cite{LokocMM2019}, have been used instead of rather vaguely defined segments.
\end{description}
With these concepts in mind, at VBS 2019 \cite{VBS2019_overview} on V3C1 1000 hour dataset \cite{V3C1dataset}, the best performing teams of experts were able to solve all ten tasks where the search clip was played to an audience, and six out of ten tasks where only a textual description was available. At VBS 2020, on the same dataset, the best team in expert session solved five out of six visual tasks and eight out of ten textual tasks. For the next years, much bigger datasets are planned; however, given the rapid pace of innovation in deep learning and other related areas, we expect to see even better results in the foreseeable future.


% In general there are two approaches to deep leaenig mocels: use only raw data as an input and target task as an output or decompose problem into myultiple sub tasks and solve each indimendenty. In video it is extract image freatures from each frame, build model on top of the features only insyteaod of high dimensional frames. In conversational chatbots it is that we decode audio into text, then create reply and again generate sound. Even in malware detection we can either use the raw files directly or create handengeneerd features and train model on top of the features only. Ususally the larger the input the more likely end to end approchs fails ... more difficult problem - no training data -> the later approcha is used. In self driing cars use create depth map from images and use it together to detect objects....

% Therefore it is no surprise that video is considered as one of the richest type of multimedia. However the richness of videos comes at great cost -- size. Compared to other types of media such as text, audio or images, video sizes are whooping -- a short clip can easily require hundred times as much space as a single image. Comparison is even starker in case of audio and text. 
% It is therefore no wonder that until relative recent advances in video compression techniques and storage video could not compete with crispness and detail of images.
%To further complicate the matter, video search can be more abstract since actions such as `chopping some vegetable' or `walking along a river' are not as precisely defined compared to standard image search objectives such as `an orange' or `the Eiffel Tower'.


\section*{Our Contribution}
\addcontentsline{toc}{section}{Our Contribution}
In this thesis, we propose, implement, and evaluate new methods and improvements in two key areas of video retrieval. Namely a shot boundary detection and text search in video or image collections. For the shot boundary detection -- a task to detect continuous video sequence captured by a single camera -- we present a state-of-the-art method based on deep learning that outperforms both standard thresholding methods as well as more recent learning-based methods on multiple public benchmarks. For text search in video collections, we improve W2VV++~\cite{XirongW2VVpp} -- a model that computes the similarity between a text and video by projecting both modalities into joint vector space using a neural network. We enrich W2VV++ with a more powerful natural language model and discuss its greatly superior performance on some tasks while achieving a bit lower performance on others.

Our work is a culmination of many years of research primarily focused on known-item search in video collections. Some of the work presented in this thesis has been published at international conferences. Aside from the already published content, the thesis contains more detailed method descriptions as well as additional experiments and ablation studies, while other aspects of known-item search are mostly left out as the sole focus of the thesis is shot boundary detection and text search. The main publications regarding the content of the thesis are the following:
\begin{enumerate}
	\item \textbf{A framework for effective known-item search in video}~\cite{LokocMM2019}\\
	Full paper describing effective approaches to known-item search. The paper also introduces TransNet shot boundary detection network. Published at ACM International Conference on Multimedia 2019 (CORE A*).
	\item \textbf{TransNet: A deep network for fast detection of common shot transitions}~\cite{soucek2019transnet}\\
	Short paper slightly extending the version published at ACM Multimedia. Published on Arxiv.
	\item \textbf{A W2VV++ Case Study with Automated and Interactive Text-to-Video Retrieval}~\cite{W2VVppMM2020}\\
	Full paper studying W2VV++ query representation learning model \cite{XirongW2VVpp} in text-based video retrieval scenarios. The paper also introduces our BERT extension to the W2VV++ model. Accepted to ACM International Conference on Multimedia 2020 (CORE A*).
	\item \textbf{TransNet V2: An effective deep network architecture for fast shot transition detection}~\cite{soucek2020transnetv2}\\
	Short paper describing our TransNet V2 model.% In review for ACM International Conference on Multimedia 2020.
\end{enumerate}
We also list some of the author's publications in the field of known-item search:
\begin{enumerate}\setcounter{enumi}{4}
	\item \textbf{Interactive Video Retrieval in the Age of Deep Learning -- Detailed Evaluation of VBS 2019}~\cite{VBS2019_overview}\\
	Journal paper analyzing the results of VBS 2019. Published in IEEE Transactions on Multimedia (IF = 6.051).
	\item \textbf{VIRET: A video retrieval tool for interactive known-item\linebreak[4] search}~\cite{LokocICMR2019}\\
	Short paper presenting our VIRET tool and showing an analysis of interaction logs from VBS 2019. Published at ACM International Conference on Multimedia Retrieval 2019.
	\item \textbf{VIRET at Video Browser Showdown 2020}~\cite{LokocVBS2020_VIRET}\\
	Demo paper describing latest version of our retrieval tool. Published at ACM International Conference on Multimedia Modeling 2020.
\end{enumerate}
Other demo papers \cite{LokocVBS2019_Nasnet,LokocLSC2019} were published on the occasions of the VBS and LSC competitions at MMM and ACM ICMR respectively. Papers \textit{Revisiting SIRET Video Retrieval Tool}~\cite{LokocVBS2018} and \textit{Using an Interactive Video Retrieval Tool for LifeLog Data}~\cite{LokocLSC2018} were already presented in the author's bachelor thesis.

We proudly report that we achieved first and second place at VBS 2018 and VBS 2019 respectively. At VBS 2020, two tools \cite{LokocVBS2020_VIRET,SOMHunterVBS2020} using our shot boundary detection method and a simplification of the W2VV++ model, as reported in \cite{W2VVppMM2020}, achieved first and second place. Furthermore, we achieved third and second place at LSC 2018 and LSC 2019 respectively.


\section*{Thesis Structure}
\addcontentsline{toc}{section}{Thesis Structure}

The thesis is divided into two chapters. The first chapter introduces methods for shot boundary detection and presents TransNet -- a neural network for shot detection (Section \ref{sec:transnetv1}). Further in the chapter, significant improvements to TransNet are made, and a new network TransNetV2 is introduced (Section \ref{sec:transnetv2}). Finally, related works are reevaluated for a fair comparison with TransNetV2 in Section \ref{sec:transnetv2_evaluation}, and an ablation study is made. The second chapter introduces approaches towards text search in image and video collections, especially the W2VV++ model by Li et al. \cite{XirongW2VVpp} (Section \ref{sec:chap2relatedwork}), and our extension W2VV++BERT is presented in Section \ref{sec:chap2ourmethod}. Both models are thoroughly evaluated together with an ablation study in Section \ref{sec:chap2experiments}.

Source code for TransNetV2, including a version with a trained model for easy integration and reevaluation, training scripts, and dataset manipulation scripts are provided as an attachment to the thesis as well as available online at \url{https://github.com/soCzech/TransNetV2}. Source code for the W2VV++BERT network, together with trained weights and details for feature extraction, are also attached to the thesis as well as available online at \url{https://github.com/soCzech/w2vvpp_bert}.


\section*{Authorship}
\addcontentsline{toc}{section}{Authorship}

All experiments presented in this thesis have been conducted solely by the author of the thesis with the only exception of the original TransNet model, described in Section \ref{sec:transnetv1}, which has been created by Mgr. Jaroslav Moravec. However, further TransNet evaluations were done by the author of this thesis. Also, the model's description in Section \ref{sec:transnetv1} as well as the paper \textit{TransNet: A deep network for fast detection of common shot transitions}~\cite{soucek2019transnet} and corresponding sections of the paper \textit{A framework for effective known-item search in video}~\cite{LokocMM2019} were written by the author of this thesis with the help of his supervisor. TransNetV2 presented in Section \ref{sec:transnetv2} has been solely the author's work.

As the work presented in this thesis has been published at international conferences, some of the thesis' content may correspond to the author's publications listed above. All such possible correspondences were written by the author of this thesis with the help of his supervisor.
